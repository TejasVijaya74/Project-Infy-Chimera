{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TejasVijaya74/Project-Infy-Chimera/blob/main/ProjectChimera.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 1 : Initial environment setup and data collection pipeline**"
      ],
      "metadata": {
        "id": "fSKdEX-zR6Lf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxoRy1_3jgAv"
      },
      "source": [
        "â€‹Step 1: Install Libraries & Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_B5BKIJh8dp",
        "outputId": "f5c8f628-06c5-40d1-c9af-9042032220a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.12/dist-packages (from newsapi-python) (2.32.4)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (2025.8.3)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n",
            "Mounted at /content/drive\n",
            "Libraries installed and Google Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install required Python libraries\n",
        "# newsapi-python: A simple Python client for the News API\n",
        "# tweepy: The official Python client for the X (formerly Twitter) API\n",
        "# pandas: Useful for data handling and viewing (optional but recommended)\n",
        "!pip install newsapi-python tweepy pandas\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from google.colab import userdata # For securely accessing API keys\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "# This will prompt you for authorization the first time you run it.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Libraries installed and Google Drive mounted successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-6zMivjuSi"
      },
      "source": [
        "Step 2: Securely Handling Your API Keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the API keys from Colab's secret manager\n",
        "news_api_key = userdata.get('NEWS_API_KEY')\n",
        "twitter_bearer_token = userdata.get('TWITTER_BEARER_TOKEN')\n",
        "\n",
        "# A quick check to ensure keys are loaded\n",
        "if news_api_key and twitter_bearer_token:\n",
        "    print(\"API keys loaded successfully.\")\n",
        "else:\n",
        "    print(\"ERROR: Could not find API keys. Please check your Colab Secrets settings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FHmu32ilvDL",
        "outputId": "688b4054-d2e4-44d1-fdc4-bc7e422cc732"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMgZ0-_qjyz8"
      },
      "source": [
        "Step 3: Data Collection from News API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2yRaSoYmig9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c8528e8-b37e-4ffb-b937-26e628d11181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Successfully collected 100 articles for 'Artificial Intelligence'.\n",
            "   Saved to: /content/drive/MyDrive/ProjectChimera/data/news/Artificial_Intelligence_2025-09-04_05-59-14.json\n"
          ]
        }
      ],
      "source": [
        "from newsapi import NewsApiClient\n",
        "\n",
        "def collect_google_news_data(api_key, query, folder_path):\n",
        "    \"\"\"\n",
        "    Collects news data for a given query and saves it to a timestamped JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the client\n",
        "        newsapi = NewsApiClient(api_key=api_key)\n",
        "\n",
        "        # Fetch the articles\n",
        "        all_articles = newsapi.get_everything(\n",
        "            q=query,\n",
        "            language='en',\n",
        "            sort_by='relevancy' # Options: relevancy, popularity, publishedAt\n",
        "        )\n",
        "\n",
        "        # Create the directory in Google Drive if it doesn't exist\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        # Create a unique, timestamped filename\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        filename = f\"{query.replace(' ', '_')}_{timestamp}.json\"\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Save the data to the file\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(all_articles['articles'], f, indent=4)\n",
        "\n",
        "        print(f\" Successfully collected {len(all_articles['articles'])} articles for '{query}'.\")\n",
        "        print(f\"   Saved to: {filepath}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" An error occurred: {e}\")\n",
        "\n",
        "# --- Let's Run It! ---\n",
        "# Define the path in your Google Drive where news data will be stored\n",
        "news_save_path = '/content/drive/MyDrive/ProjectChimera/data/news'\n",
        "# Define your search query\n",
        "search_query = \"Artificial Intelligence\"\n",
        "\n",
        "collect_google_news_data(news_api_key, search_query, news_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncnO5Pfqj7KQ"
      },
      "source": [
        "Step 4: Data Collection from X (Twitter) API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fo_uZAqFisBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab84f38-62f8-4197-ee72-4cfa74abc06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " An error occurred: 429 Too Many Requests\n",
            "Usage cap exceeded: Monthly product cap\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "\n",
        "def collect_twitter_data(bearer_token, query, folder_path):\n",
        "    \"\"\"\n",
        "    Collects recent tweets for a given query and saves them to a timestamped JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the client\n",
        "        client = tweepy.Client(bearer_token)\n",
        "\n",
        "        # Fetch recent tweets (free tier allows searching the last 7 days)\n",
        "        response = client.search_recent_tweets(\n",
        "            query=f\"{query} -is:retweet\",  # Search query, excluding retweets\n",
        "            max_results=100,              # Max results per request (10-100)\n",
        "            tweet_fields=[\"created_at\", \"public_metrics\", \"lang\"]\n",
        "        )\n",
        "\n",
        "        if not response.data:\n",
        "            print(f\" No tweets found for the query: '{query}'\")\n",
        "            return\n",
        "\n",
        "        # Prepare data for saving\n",
        "        tweets_to_save = []\n",
        "        for tweet in response.data:\n",
        "            tweets_to_save.append({\n",
        "                'id': tweet.id,\n",
        "                'text': tweet.text,\n",
        "                'created_at': str(tweet.created_at),\n",
        "                'retweet_count': tweet.public_metrics['retweet_count'],\n",
        "                'reply_count': tweet.public_metrics['reply_count'],\n",
        "                'like_count': tweet.public_metrics['like_count'],\n",
        "                'impression_count': tweet.public_metrics['impression_count']\n",
        "            })\n",
        "\n",
        "        # Create the directory in Google Drive if it doesn't exist\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        # Create a unique, timestamped filename\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        filename = f\"{query.replace(' ', '_')}_{timestamp}.json\"\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Save the data to the file\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(tweets_to_save, f, indent=4)\n",
        "\n",
        "        print(f\" Successfully collected {len(tweets_to_save)} tweets for '{query}'.\")\n",
        "        print(f\"   Saved to: {filepath}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" An error occurred: {e}\")\n",
        "\n",
        "# --- Let's Run It! ---\n",
        "# Define the path in your Google Drive where twitter data will be stored\n",
        "twitter_save_path = '/content/drive/MyDrive/ProjectChimera/data/twitter'\n",
        "# Define your search query (can be the same or different)\n",
        "search_query = \"Artificial Intelligence\"\n",
        "\n",
        "collect_twitter_data(twitter_bearer_token, search_query, twitter_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 2 : Building the Trend & Alert System**\n",
        "\n"
      ],
      "metadata": {
        "id": "PMljIeORIifx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Step 1: Installations and Secure Setup"
      ],
      "metadata": {
        "id": "J-B5pDBuJGhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install VADER for fast, rule-based sentiment analysis\n",
        "# It's great for this stage because it's simple and effective.\n",
        "!pip install vaderSentiment\n",
        "\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# --- SECURE SETUP ---\n",
        "# 1. Go to the key icon (ðŸ”‘) in the Colab sidebar.\n",
        "# 2. Add a new secret:\n",
        "#    Name: SLACK_WEBHOOK_URL\n",
        "#    Value: [Paste your Slack Webhook URL here]\n",
        "# 3. Make sure \"Notebook access\" is toggled ON.\n",
        "\n",
        "SLACK_URL = userdata.get('SLACK_WEBHOOK_URL')\n",
        "print(\" Setup complete. VADER installed and Slack URL loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIiVLkIuIh0d",
        "outputId": "79466c8c-e6b1-4200-86e0-4233975bc5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.12/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.8.3)\n",
            " Setup complete. VADER installed and Slack URL loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Step: Testing Your Alert System (optional)"
      ],
      "metadata": {
        "id": "XaGTneSwMo74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ALERTING PARAMETERS (Temporarily adjusted for testing!) ---\n",
        "KEYWORDS_TO_TRACK = {\n",
        "    'funding': 3,      # Lowered from 5 to 3 (will now trigger an alert)\n",
        "    'partnership': 1,  # Lowered from 3 to 1 (will now trigger an alert)\n",
        "    'layoff': 0,\n",
        "}\n",
        "\n",
        "# Lower the sentiment threshold to a positive value to trigger it\n",
        "SENTIMENT_THRESHOLD = 0.2\n",
        "# --- HELPER FUNCTIONS ---\n",
        "\n",
        "def send_slack_alert(message):\n",
        "    \"\"\"Sends a formatted message to our Slack channel.\"\"\"\n",
        "    if not SLACK_URL:\n",
        "        print(\" Slack URL not found. Cannot send alert.\")\n",
        "        return\n",
        "    payload = {'text': message}\n",
        "    try:\n",
        "        response = requests.post(SLACK_URL, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            print(\" Slack alert sent successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error sending Slack alert: {e}\")\n",
        "\n",
        "def analyze_sentiment_vader(text):\n",
        "    \"\"\"Analyzes sentiment of a text using VADER.\"\"\"\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    # The 'compound' score is a single metric from -1 (most negative) to +1 (most positive)\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']"
      ],
      "metadata": {
        "id": "RfIf_DUsMoSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Define the Alerting Logic and Helper Functions"
      ],
      "metadata": {
        "id": "u5pQSumYLJT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ALERTING PARAMETERS (You can tune these!) ---\n",
        "# Define keywords that signal important strategic events\n",
        "KEYWORDS_TO_TRACK = {\n",
        "    'funding': 5,      # Alert if 'funding' is mentioned > 5 times\n",
        "    'partnership': 3,  # Alert if 'partnership' is mentioned > 3 times\n",
        "    'layoff': 2,       # Alert if 'layoff' is mentioned > 2 times\n",
        "}\n",
        "\n",
        "# Define a sentiment threshold\n",
        "# We'll alert if the average sentiment drops below -0.1 (leaning negative)\n",
        "SENTIMENT_THRESHOLD = -0.1\n",
        "\n",
        "# --- HELPER FUNCTIONS ---\n",
        "\n",
        "def send_slack_alert(message):\n",
        "    \"\"\"Sends a formatted message to our Slack channel.\"\"\"\n",
        "    if not SLACK_URL:\n",
        "        print(\" Slack URL not found. Cannot send alert.\")\n",
        "        return\n",
        "    payload = {'text': message}\n",
        "    try:\n",
        "        response = requests.post(SLACK_URL, json=payload)\n",
        "        if response.status_code == 200:\n",
        "            print(\" Slack alert sent successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\" Error sending Slack alert: {e}\")\n",
        "\n",
        "def analyze_sentiment_vader(text):\n",
        "    \"\"\"Analyzes sentiment of a text using VADER.\"\"\"\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    # The 'compound' score is a single metric from -1 (most negative) to +1 (most positive)\n",
        "    sentiment = analyzer.polarity_scores(text)\n",
        "    return sentiment['compound']"
      ],
      "metadata": {
        "id": "abptV2N_LLA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: The Main Function to Check for Trends and Send Alerts"
      ],
      "metadata": {
        "id": "tsXKGpYyLf8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_for_trends_and_alerts():\n",
        "    \"\"\"\n",
        "    Main function to process recent data and trigger alerts.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Starting Trend Analysis ---\\n\")\n",
        "\n",
        "    # --- 1. Load Recent Data (from the last 24 hours) ---\n",
        "    all_text = []\n",
        "    data_root_path = '/content/drive/MyDrive/ProjectChimera/data'\n",
        "    yesterday = datetime.now() - timedelta(days=1)\n",
        "\n",
        "    for data_type in ['news', 'twitter']:\n",
        "        folder_path = os.path.join(data_root_path, data_type)\n",
        "        if not os.path.exists(folder_path): continue\n",
        "\n",
        "        for filename in os.listdir(folder_path):\n",
        "            try:\n",
        "                # Check if the file was created in the last 24 hours\n",
        "                file_timestamp_str = filename.split('_')[-2] + \"_\" + filename.split('_')[-1].split('.')[0]\n",
        "                file_timestamp = datetime.strptime(file_timestamp_str, '%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "                if file_timestamp > yesterday:\n",
        "                    filepath = os.path.join(folder_path, filename)\n",
        "                    with open(filepath, 'r') as f:\n",
        "                        data = json.load(f)\n",
        "                        # Extract text from both news and twitter data structures\n",
        "                        for item in data:\n",
        "                            all_text.append(item.get('title', '') + \" \" + item.get('description', '') if data_type == 'news' else item.get('text', ''))\n",
        "            except Exception as e:\n",
        "                # This handles cases where filename format might be different\n",
        "                continue\n",
        "\n",
        "    if not all_text:\n",
        "        print(\"No new data from the last 24 hours to analyze.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Found {len(all_text)} new items from the last 24 hours.\")\n",
        "    full_text_corpus = \" \".join(all_text).lower()\n",
        "\n",
        "    # --- 2. Analyze for Keyword Surges ---\n",
        "    print(\"\\n Analyzing for keyword surges...\")\n",
        "    for keyword, threshold in KEYWORDS_TO_TRACK.items():\n",
        "        count = full_text_corpus.count(keyword)\n",
        "        print(f\"  - Found '{keyword}' {count} times (Threshold: {threshold})\")\n",
        "        if count > threshold:\n",
        "            alert_message = (f\"ðŸš¨ *Keyword Surge Alert!* ðŸš¨\\n\"\n",
        "                             f\"> The keyword `*{keyword}*` was mentioned `{count}` times in the last 24 hours, \"\n",
        "                             f\"exceeding the threshold of `{threshold}`.\\n\"\n",
        "                             f\"> This may indicate a significant market event.\")\n",
        "            send_slack_alert(alert_message)\n",
        "\n",
        "    # --- 3. Analyze for Sentiment Shifts ---\n",
        "    print(\"\\n Analyzing for sentiment shifts...\")\n",
        "    total_sentiment_score = 0\n",
        "    for text in all_text:\n",
        "        total_sentiment_score += analyze_sentiment_vader(text)\n",
        "\n",
        "    average_sentiment = total_sentiment_score / len(all_text) if all_text else 0\n",
        "    print(f\"  - Average sentiment score: {average_sentiment:.4f} (Threshold: {SENTIMENT_THRESHOLD})\")\n",
        "\n",
        "    if average_sentiment < SENTIMENT_THRESHOLD:\n",
        "        alert_message = (f\" *Negative Sentiment Alert!* \\n\"\n",
        "                         f\"> The average sentiment score in the last 24 hours was `{average_sentiment:.4f}`, \"\n",
        "                         f\"which is below the threshold of `{SENTIMENT_THRESHOLD}`.\\n\"\n",
        "                         f\"> This may indicate negative public perception or bad news.\")\n",
        "        send_slack_alert(alert_message)\n",
        "\n",
        "    print(\"\\n--- Analysis Complete ---\")\n",
        "\n",
        "# --- Let's Run It! ---\n",
        "# This will perform the check on the data you've collected.\n",
        "check_for_trends_and_alerts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f39zKhFNLkXo",
        "outputId": "c6c4e4de-77aa-4593-abf5-49d52ee7aa91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Trend Analysis ---\n",
            "\n",
            "Found 397 new items from the last 24 hours.\n",
            "\n",
            " Analyzing for keyword surges...\n",
            "  - Found 'funding' 4 times (Threshold: 3)\n",
            " Slack alert sent successfully!\n",
            "  - Found 'partnership' 2 times (Threshold: 1)\n",
            " Slack alert sent successfully!\n",
            "  - Found 'layoff' 0 times (Threshold: 0)\n",
            "\n",
            " Analyzing for sentiment shifts...\n",
            "  - Average sentiment score: 0.2646 (Threshold: 0.2)\n",
            "\n",
            "--- Analysis Complete ---\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "PMljIeORIifx"
      ],
      "authorship_tag": "ABX9TyN1I14DLWuZG2ir9ez3zCqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TejasVijaya74/Project-Infy-Chimera/blob/main/Task1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Phase 1 : Initial environment setup and data collection pipeline**"
      ],
      "metadata": {
        "id": "fSKdEX-zR6Lf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxoRy1_3jgAv"
      },
      "source": [
        "â€‹Step 1: Install Libraries & Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_B5BKIJh8dp",
        "outputId": "5ee7dacb-94dd-4488-89c7-352c8a910a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newsapi-python\n",
            "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.12/dist-packages (4.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests<3.0.0 in /usr/local/lib/python3.12/dist-packages (from newsapi-python) (2.32.4)\n",
            "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (3.3.1)\n",
            "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from tweepy) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0->newsapi-python) (2025.8.3)\n",
            "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: newsapi-python\n",
            "Successfully installed newsapi-python-0.2.7\n",
            "Mounted at /content/drive\n",
            "Libraries installed and Google Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Install required Python libraries\n",
        "# newsapi-python: A simple Python client for the News API\n",
        "# tweepy: The official Python client for the X (formerly Twitter) API\n",
        "# pandas: Useful for data handling and viewing (optional but recommended)\n",
        "!pip install newsapi-python tweepy pandas\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from google.colab import userdata # For securely accessing API keys\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "# This will prompt you for authorization the first time you run it.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Libraries installed and Google Drive mounted successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v-6zMivjuSi"
      },
      "source": [
        "Step 2: Securely Handling Your API Keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the API keys from Colab's secret manager\n",
        "news_api_key = userdata.get('NEWS_API_KEY')\n",
        "twitter_bearer_token = userdata.get('TWITTER_BEARER_TOKEN')\n",
        "\n",
        "# A quick check to ensure keys are loaded\n",
        "if news_api_key and twitter_bearer_token:\n",
        "    print(\"API keys loaded successfully.\")\n",
        "else:\n",
        "    print(\"ERROR: Could not find API keys. Please check your Colab Secrets settings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FHmu32ilvDL",
        "outputId": "1bc8ea76-d67e-42bd-a33d-aba4531ded97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API keys loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMgZ0-_qjyz8"
      },
      "source": [
        "Step 3: Data Collection from News API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2yRaSoYmig9r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0b64c9-405f-463c-8e38-25b1e4987998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Successfully collected 100 articles for 'Artificial Intelligence'.\n",
            "   Saved to: /content/drive/MyDrive/ProjectChimera/data/news/Artificial_Intelligence_2025-09-08_13-31-20.json\n"
          ]
        }
      ],
      "source": [
        "from newsapi import NewsApiClient\n",
        "\n",
        "def collect_google_news_data(api_key, query, folder_path):\n",
        "    \"\"\"\n",
        "    Collects news data for a given query and saves it to a timestamped JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the client\n",
        "        newsapi = NewsApiClient(api_key=api_key)\n",
        "\n",
        "        # Fetch the articles\n",
        "        all_articles = newsapi.get_everything(\n",
        "            q=query,\n",
        "            language='en',\n",
        "            sort_by='relevancy' # Options: relevancy, popularity, publishedAt\n",
        "        )\n",
        "\n",
        "        # Create the directory in Google Drive if it doesn't exist\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        # Create a unique, timestamped filename\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        filename = f\"{query.replace(' ', '_')}_{timestamp}.json\"\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Save the data to the file\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(all_articles['articles'], f, indent=4)\n",
        "\n",
        "        print(f\" Successfully collected {len(all_articles['articles'])} articles for '{query}'.\")\n",
        "        print(f\"   Saved to: {filepath}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" An error occurred: {e}\")\n",
        "\n",
        "# --- Let's Run It! ---\n",
        "# Define the path in your Google Drive where news data will be stored\n",
        "news_save_path = '/content/drive/MyDrive/ProjectChimera/data/news'\n",
        "# Define your search query\n",
        "search_query = \"Artificial Intelligence\"\n",
        "\n",
        "collect_google_news_data(news_api_key, search_query, news_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncnO5Pfqj7KQ"
      },
      "source": [
        "Step 4: Data Collection from X (Twitter) API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fo_uZAqFisBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4df2b6-3530-4475-bd9f-c6bd1793e72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " An error occurred: 429 Too Many Requests\n",
            "Usage cap exceeded: Monthly product cap\n"
          ]
        }
      ],
      "source": [
        "import tweepy\n",
        "\n",
        "def collect_twitter_data(bearer_token, query, folder_path):\n",
        "    \"\"\"\n",
        "    Collects recent tweets for a given query and saves them to a timestamped JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize the client\n",
        "        client = tweepy.Client(bearer_token)\n",
        "\n",
        "        # Fetch recent tweets (free tier allows searching the last 7 days)\n",
        "        response = client.search_recent_tweets(\n",
        "            query=f\"{query} -is:retweet\",  # Search query, excluding retweets\n",
        "            max_results=100,              # Max results per request (10-100)\n",
        "            tweet_fields=[\"created_at\", \"public_metrics\", \"lang\"]\n",
        "        )\n",
        "\n",
        "        if not response.data:\n",
        "            print(f\" No tweets found for the query: '{query}'\")\n",
        "            return\n",
        "\n",
        "        # Prepare data for saving\n",
        "        tweets_to_save = []\n",
        "        for tweet in response.data:\n",
        "            tweets_to_save.append({\n",
        "                'id': tweet.id,\n",
        "                'text': tweet.text,\n",
        "                'created_at': str(tweet.created_at),\n",
        "                'retweet_count': tweet.public_metrics['retweet_count'],\n",
        "                'reply_count': tweet.public_metrics['reply_count'],\n",
        "                'like_count': tweet.public_metrics['like_count'],\n",
        "                'impression_count': tweet.public_metrics['impression_count']\n",
        "            })\n",
        "\n",
        "        # Create the directory in Google Drive if it doesn't exist\n",
        "        os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "        # Create a unique, timestamped filename\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "        filename = f\"{query.replace(' ', '_')}_{timestamp}.json\"\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Save the data to the file\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(tweets_to_save, f, indent=4)\n",
        "\n",
        "        print(f\" Successfully collected {len(tweets_to_save)} tweets for '{query}'.\")\n",
        "        print(f\"   Saved to: {filepath}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" An error occurred: {e}\")\n",
        "\n",
        "# --- Let's Run It! ---\n",
        "# Define the path in your Google Drive where twitter data will be stored\n",
        "twitter_save_path = '/content/drive/MyDrive/ProjectChimera/data/twitter'\n",
        "# Define your search query (can be the same or different)\n",
        "search_query = \"Artificial Intelligence\"\n",
        "\n",
        "collect_twitter_data(twitter_bearer_token, search_query, twitter_save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "PMljIeORIifx"
      ],
      "authorship_tag": "ABX9TyODzeMjOp3O+T2fOhEXAMTs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}